###################### Filebeat Configuration Example #########################

# This file is an example configuration file highlighting only the most common
# options. The filebeat.reference.yml file from the same directory contains all the
# supported options with more comments. You can use it as a reference.
#
# You can find the full configuration reference here:
# https://www.elastic.co/guide/en/beats/filebeat/index.html

# For more available modules and options, please see the filebeat.reference.yml sample
# configuration file.

# ============================== Filebeat inputs ===============================

filebeat.inputs:

# Each - is an input. Most options can be set at the input level, so
# you can use different inputs for various configurations.
# Below are the input specific configurations.

# Application logs
- type: log
  enabled: true
  paths:
    - /var/log/blog-system/*.log
    - /var/log/blog-system/**/*.log
  fields:
    service: blog-system
    environment: ${ENVIRONMENT:development}
    log_type: application
  fields_under_root: true
  multiline.pattern: '^\d{4}-\d{2}-\d{2}'
  multiline.negate: true
  multiline.match: after
  json.keys_under_root: true
  json.add_error_key: true
  json.message_key: message
  processors:
    - add_host_metadata:
        when.not.contains.tags: forwarded
    - add_docker_metadata: ~
    - add_kubernetes_metadata: ~

# Nginx access logs
- type: log
  enabled: true
  paths:
    - /var/log/nginx/access.log
  fields:
    service: nginx
    log_type: access
  fields_under_root: true
  processors:
    - dissect:
        tokenizer: '%{remote_addr} - %{remote_user} [%{time_local}] "%{method} %{url} %{protocol}" %{status} %{body_bytes_sent} "%{http_referer}" "%{http_user_agent}" "%{http_x_forwarded_for}"'
        field: "message"
        target_prefix: "nginx"
    - timestamp:
        field: nginx.time_local
        layouts:
          - '02/Jan/2006:15:04:05 -0700'
        test:
          - '25/May/2023:10:05:32 +0000'
    - convert:
        fields:
          - {from: "nginx.status", to: "nginx.status_code", type: "integer"}
          - {from: "nginx.body_bytes_sent", to: "nginx.bytes_sent", type: "integer"}
        ignore_missing: true
    - add_host_metadata:
        when.not.contains.tags: forwarded

# Nginx error logs
- type: log
  enabled: true
  paths:
    - /var/log/nginx/error.log
  fields:
    service: nginx
    log_type: error
  fields_under_root: true
  multiline.pattern: '^\d{4}/\d{2}/\d{2}'
  multiline.negate: true
  multiline.match: after
  processors:
    - dissect:
        tokenizer: '%{timestamp} [%{level}] %{pid}#%{tid}: %{message}'
        field: "message"
        target_prefix: "nginx"
    - add_host_metadata:
        when.not.contains.tags: forwarded

# Docker container logs
- type: container
  enabled: true
  paths:
    - '/var/lib/docker/containers/*/*.log'
  processors:
    - add_docker_metadata:
        host: "unix:///var/run/docker.sock"
        match_fields: ["container.id"]
        match_pids: ["process.pid"]
        match_source: true
        match_source_index: 4
        match_short_id: false
        cleanup_timeout: 60
        labels.dedot: false
        # To connect to Docker over TLS you must specify a client and CA certificate.
        #ssl:
        #  certificate_authorities: ["/etc/pki/root/ca.pem"]
        #  certificate:              "/etc/pki/client/cert.pem"
        #  key:                      "/etc/pki/client/cert.key"
    - decode_json_fields:
        fields: ["message"]
        target: ""
        overwrite_keys: true
    - timestamp:
        field: "@timestamp"
        layouts:
          - '2006-01-02T15:04:05.000000000Z'
          - '2006-01-02T15:04:05Z'
        test:
          - '2023-05-25T10:05:32.123456789Z'

# System logs (syslog)
- type: syslog
  enabled: true
  protocol.udp:
    host: "0.0.0.0:514"
  fields:
    service: system
    log_type: syslog
  fields_under_root: true

# Journal logs (systemd)
- type: journald
  enabled: true
  id: everything
  fields:
    service: systemd
    log_type: journal
  fields_under_root: true
  include_matches:
    - "_SYSTEMD_UNIT=docker.service"
    - "_SYSTEMD_UNIT=nginx.service"
    - "_SYSTEMD_UNIT=postgresql.service"
    - "_SYSTEMD_UNIT=redis.service"

# ============================== Filebeat modules ===============================

filebeat.config.modules:
  # Glob pattern for configuration loading
  path: ${path.config}/modules.d/*.yml

  # Set to true to enable config reloading
  reload.enabled: true

  # Period on which files under path should be checked for changes
  reload.period: 10s

# ======================= Elasticsearch template setting =======================

setup.template.settings:
  index.number_of_shards: 1
  index.number_of_replicas: 0
  index.codec: best_compression
  _source.enabled: true

# ================================== General ===================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
name: blog-system-filebeat

# The tags of the shipper are included in their own field with each
# transaction published.
tags: ["blog-system", "filebeat", "${ENVIRONMENT:development}"]

# Optional fields that you can specify to add additional information to the
# output.
fields:
  environment: ${ENVIRONMENT:development}
  datacenter: ${DATACENTER:local}
  project: blog-system

fields_under_root: false

# ================================= Dashboards =================================
# These settings control loading the sample dashboards to the Kibana index. Loading
# the dashboards is disabled by default and can be enabled either by setting the
# options here or by using the `setup` command.
setup.dashboards.enabled: true

# The URL from where to download the dashboards archive. By default this URL
# has a value which is computed based on the Beat name and version. For released
# versions, this URL points to the dashboard archive on the artifacts.elastic.co
# website.
#setup.dashboards.url:

# =================================== Kibana ===================================

# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.
# This requires a Kibana endpoint configuration.
setup.kibana:
  # Kibana Host
  # Scheme and port can be left out and will be set to the default (http and 5601)
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601
  host: "kibana:5601"

  # Kibana Space ID
  # ID of the Kibana Space into which the dashboards should be loaded. By default,
  # the Default Space will be used.
  #space.id:

# =============================== Elastic Cloud ================================

# These settings simplify using Filebeat with the Elastic Cloud (https://cloud.elastic.co/).

# The cloud.id setting overwrites the `output.elasticsearch.hosts` and
# `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
#cloud.id:

# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `<user>:<pass>`.
#cloud.auth:

# ================================== Outputs ===================================

# Configure what output to use when sending the data collected by the beat.

# ---------------------------- Elasticsearch Output ----------------------------
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["elasticsearch:9200"]

  # Protocol - either `http` (default) or `https`.
  #protocol: "https"

  # Authentication credentials - either API key or username/password.
  #api_key: "id:api_key"
  #username: "elastic"
  #password: "changeme"

  # Index name
  index: "filebeat-%{[agent.version]}-%{+yyyy.MM.dd}"

  # Template settings
  template.name: "filebeat"
  template.pattern: "filebeat-*"
  template.settings:
    index.number_of_shards: 1
    index.number_of_replicas: 0
    index.codec: best_compression

  # ILM settings
  ilm.enabled: true
  ilm.rollover_alias: "filebeat"
  ilm.pattern: "{now/d}-000001"
  ilm.policy: "filebeat-policy"

# ------------------------------ Logstash Output -------------------------------
#output.logstash:
  # The Logstash hosts
  #hosts: ["logstash:5044"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

# ================================= Processors =================================

# Configure processors to enhance or manipulate events generated by the beat.

processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
  - add_kubernetes_metadata: ~
  - fingerprint:
      fields: ["message", "@timestamp", "host.name"]
      target_field: "@metadata.fingerprint"
  - drop_fields:
      fields: ["agent.ephemeral_id", "agent.id", "ecs.version", "input.type"]
      ignore_missing: true

# ================================== Logging ===================================

# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
logging.level: info

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors use ["*"]. Examples of other selectors are "beat",
# "publisher", "service".
#logging.selectors: ["*"]

# Logging to rotating files
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
  rotateeverybytes: 10485760 # = 10MB

# Logging to syslog
#logging.to_syslog: true
#logging.syslog:
  #facility: local0

# ============================= X-Pack Monitoring ==============================
# Filebeat can export internal metrics to a central Elasticsearch monitoring
# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The
# reporting is disabled by default.

# Set to true to enable the monitoring reporter.
monitoring.enabled: true

# Sets the UUID of the Elasticsearch cluster under which monitoring data for this
# Filebeat instance will appear in the Stack Monitoring UI. If output.elasticsearch
# is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch.
#monitoring.cluster_uuid:

# Uncomment to send the metrics to Elasticsearch. Most settings from the
# Elasticsearch output are accepted here as well.
# Note that the settings should point to your Elasticsearch *monitoring* cluster.
# Any setting that is not set is automatically inherited from the Elasticsearch
# output configuration, so if you have the Elasticsearch output configured such
# that it is pointing to your Elasticsearch monitoring cluster, you can simply
# uncomment the following line.
monitoring.elasticsearch:
  hosts: ["elasticsearch:9200"]
  #username: monitoring_user
  #password: monitoring_pass

# ============================== Instrumentation ===============================

# Instrumentation support for the filebeat.
instrumentation:
  # Set to true to enable instrumentation of filebeat.
  enabled: false

  # Environment in which filebeat is running on (eg: staging, production, etc.)
  environment: "${ENVIRONMENT:development}"

  # APM Server hosts to report instrumentation results to.
  hosts:
    - http://apm-server:8200

  # API Key for the APM Server(s).
  # If api_key is set then secret_token will be ignored.
  #api_key:

  # Secret token for the APM Server(s).
  #secret_token:

# ================================= Migration ==================================

# This allows to enable 6.7 migration aliases
migration.6_to_7.enabled: false

# ================================== HTTP Endpoint =============================

# Each beat can expose internal metrics through a HTTP endpoint. For security
# reasons the endpoint is disabled by default. This feature is currently experimental.
# Stats can be access through http://localhost:5066/stats . For pretty JSON add
# ?pretty to the URL.

# Defines if the HTTP endpoint is enabled.
http.enabled: true

# The HTTP endpoint will bind to this hostname, IP address, unix socket or named pipe.
# When using IP addresses, it is recommended to only use localhost.
http.host: "0.0.0.0"

# Port on which the HTTP endpoint will bind. Default is 5066.
http.port: 5066

# Define which user should be owning the named pipe.
#http.named_pipe.user:

# Define which the permissions that should be applied to the named pipe, use the Security
# Descriptor Definition Language (SDDL) to define the permission. This option cannot be used with
# `http.named_pipe.user`.
#http.named_pipe.security_descriptor:

# ============================== Feature Flags ================================

# Experimental features are disabled by default. These feature flags enable them.
# Warning: experimental features are subject to change or removal in future releases.
# Do not use these features in production.

#feature_flags:
  #fqdn.enabled: false

# ============================== Autodiscover ==================================

# Autodiscover allows you to detect changes in the system and spawn new modules or inputs as they happen.

filebeat.autodiscover:
  providers:
    - type: docker
      hints.enabled: true
      hints.default_config:
        type: container
        paths:
          - /var/lib/docker/containers/${data.docker.container.id}/*.log
      templates:
        - condition:
            contains:
              docker.container.image: nginx
          config:
            - type: container
              paths:
                - /var/lib/docker/containers/${data.docker.container.id}/*.log
              processors:
                - dissect:
                    tokenizer: '%{remote_addr} - %{remote_user} [%{time_local}] "%{method} %{url} %{protocol}" %{status} %{body_bytes_sent} "%{http_referer}" "%{http_user_agent}"'
                    field: "message"
                    target_prefix: "nginx"
        - condition:
            contains:
              docker.container.image: blog-system
          config:
            - type: container
              paths:
                - /var/lib/docker/containers/${data.docker.container.id}/*.log
              json.keys_under_root: true
              json.add_error_key: true
              json.message_key: message
    - type: kubernetes
      node: ${NODE_NAME}
      hints.enabled: true
      hints.default_config:
        type: container
        paths:
          - /var/log/containers/*${data.kubernetes.container.id}.log
      templates:
        - condition:
            contains:
              kubernetes.labels.app: blog-system
          config:
            - type: container
              paths:
                - /var/log/containers/*${data.kubernetes.container.id}.log
              json.keys_under_root: true
              json.add_error_key: true
              json.message_key: message
              processors:
                - add_kubernetes_metadata:
                    host: ${NODE_NAME}
                    matchers:
                    - logs_path:
                        logs_path: "/var/log/containers/"

# ============================== Keystore ====================================

# Location of the Keystore containing the keys and their sensitive values.
#keystore.path: "${path.config}/filebeat.keystore"